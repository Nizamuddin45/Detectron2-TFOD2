{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is Detectron2 and how does it differ from previous object detection frameworks?\n",
        "\n",
        " -  Detectron2 is an open-source object detection and segmentation framework by Meta (Facebook AI Research). It is built on PyTorch and is commonly used in Google Colab for tasks like object detection, instance segmentation, semantic segmentation, and keypoint detection because it is easy to install and experiment with.\n",
        "\n",
        " - Compared to older object detection frameworks, Detectron2 is more modern and user-friendly. Earlier frameworks like Detectron (v1) were based on Caffe2, which made debugging and customization difficult, especially in Colab environments. Detectron2 being PyTorch-based allows easier coding, better debugging, and smooth GPU usage in Colab.\n",
        "\n",
        " - Detectron2 also has a modular architecture, so components like backbone networks, ROI heads, and loss functions can be easily modified. It provides a rich model zoo with pretrained models (such as Faster R-CNN and Mask R-CNN), which is very helpful in Colab where training from scratch is time-consuming. Overall, Detectron2 is faster, easier to experiment with, and better suited for research and prototyping in Google Colab than previous object detection frameworks.\n",
        " ---"
      ],
      "metadata": {
        "id": "LDRUv6YxPoUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:**   Explain the process and importance of data annotation when working with Detectron2.\n",
        "\n",
        " -  Data annotation is a crucial step when working with Detectron2 because it is a supervised learning framework, meaning the model learns only from labeled data. In Google Colab projects, proper annotation helps the model understand what objects are present in an image and where they are located.\n",
        "\n",
        " - The process starts by collecting images related to the task. Then, based on the problem (object detection, instance segmentation, or keypoint detection), the images are annotated using tools like LabelImg, CVAT, or Roboflow. These tools allow drawing bounding boxes, segmentation masks, or keypoints and assigning class labels to each object.\n",
        "\n",
        " - Detectron2 mainly works with COCO-format annotations, so the annotated data is converted into a COCO-style JSON file. This file contains image details, class information, and object coordinates. After that, the dataset is registered in Detectron2, so it can be used for training and evaluation in Google Colab.\n",
        "\n",
        " - Data annotation is important because accurate labels improve model accuracy, help in correct object localization, reduce training errors, and allow the model to generalize well on new images. Poor or incorrect annotation can lead to wrong predictions, even if a powerful model is used.\n",
        "---"
      ],
      "metadata": {
        "id": "N78LtOLlQCr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Describe the steps involved in training a custom object detection model using Detectron2.\n",
        "\n",
        " -  Training a custom object detection model using Detectron2 in Google Colab follows a clear and structured pipeline. The main steps are given below.\n",
        "\n",
        " - First, install Detectron2 and set up the environment in Google Colab. This includes enabling GPU and installing the correct Detectron2 version compatible with PyTorch and CUDA.\n",
        "\n",
        " - Next, prepare and annotate the dataset. Images are labeled with bounding boxes (or masks) using annotation tools, and the annotations are converted into COCO format. After this, the dataset is registered in Detectron2 so it can be loaded during training.\n",
        "\n",
        " - Then, select a pretrained model from the Detectron2 Model Zoo (such as Faster R-CNN or Mask R-CNN). Using pretrained weights helps in faster convergence and better performance, especially in Colab where training time is limited.\n",
        "\n",
        " - After that, configure the training parameters using the Detectron2 config file. This includes setting the dataset name, number of classes, learning rate, batch size, and number of training iterations.\n",
        "\n",
        " - Once the configuration is ready, the model is trained using the DefaultTrainer. During training, Detectron2 automatically handles data loading, loss calculation, and optimization.\n",
        "\n",
        " - Finally, evaluate and test the trained model on validation or test images to check detection accuracy. The trained weights can then be saved and used for inference on new images.\n",
        "\n",
        " - In summary, the process includes environment setup, data preparation, model selection, configuration, training, and evaluation, making Detectron2 suitable for custom object detection tasks in Google Colab.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YCuMViBiQbDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?\n",
        "\n",
        "\n",
        " -  In Detectron2, evaluation curves and metrics are used to measure how well an object detection model is performing on validation or test data. These help us understand both detection accuracy and localization quality of the model.\n",
        "\n",
        " - Evaluation curves mainly include Precision–Recall (PR) curves. A PR curve shows the trade-off between precision (how many detected objects are correct) and recall (how many actual objects were detected). In Detectron2, these curves are automatically generated during evaluation using COCO-style evaluation.\n",
        "\n",
        " - IoU (Intersection over Union) measures how much the predicted bounding box overlaps with the ground-truth box.\n",
        "\n",
        " - IoU= (Area of Overlap) / (Area of Union)\n",
        "\t​\n",
        " - An IoU value closer to 1 means better localization. Common thresholds are 0.5 or 0.75, where detections below the threshold are considered incorrect.\n",
        "\n",
        " - mAP (mean Average Precision) is the main performance metric in Detectron2. It is computed by averaging the Average Precision (AP) across all object classes and multiple IoU thresholds (from 0.5 to 0.95 in COCO evaluation). Higher mAP values indicate better overall detection and localization performance.\n",
        "\n",
        " - In simple terms, IoU checks how well objects are localized, while mAP summarizes the overall detection quality of the model. These metrics together help decide whether a Detectron2 model is reliable for real-world use.\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "0sU-MTBcQ0DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Compare Detectron2 and TFOD2 in terms of features, performance, and ease of use.\n",
        "  -  **Features**\n",
        "\n",
        "  | Feature               | **Detectron2**                                               | **TFOD2 (TensorFlow Object Detection)**          |\n",
        "| --------------------- | ------------------------------------------------------------ | ------------------------------------------------ |\n",
        "| Framework             | PyTorch                                                      | TensorFlow 2.x                                   |\n",
        "| Models                | Faster R-CNN, Mask R-CNN, RetinaNet, Cascade, Keypoint R-CNN | SSD, Faster R-CNN, EfficientDet, CenterNet, etc. |\n",
        "| Instance Segmentation | ✔️ (strong support)                                          | ✔️ (via models)                                  |\n",
        "| Keypoint / Pose       | ✔️                                                           | Limited support                                  |\n",
        "| Model Zoo             | Rich & research-oriented                                     | Very broad, production + research                |\n",
        "| Export formats        | TorchScript, ONNX                                            | SavedModel, TFLite, TF Hub                       |\n",
        "\n",
        " - **Performance**\n",
        "\n",
        " | Performance Aspect | **Detectron2**                 | **TFOD2**                   |\n",
        "| ------------------ | ------------------------------ | --------------------------- |\n",
        "| Accuracy (mAP)     | Higher (research-grade models) | Good, slightly lower        |\n",
        "| Inference Speed    | Moderate (heavy models)        | Faster (SSD, EfficientDet)  |\n",
        "| Training Speed     | Slower                         | Faster                      |\n",
        "| GPU Memory Use     | High                           | Low–Medium                  |\n",
        "| Scalability        | Good on strong GPUs            | Better on limited resources |\n",
        "| Colab Suitability  | Needs more GPU memory          | Works well on Colab         |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3PsSZZ3xRg1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Write Python code to install Detectron2 and verify the installation.\n",
        "  "
      ],
      "metadata": {
        "id": "gBNOSNi8SGB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 1: Check PyTorch\n",
        "# ================================\n",
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ================================\n",
        "# Step 2: Install Detectron2 (generic)\n",
        "# ================================\n",
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade pip\n",
        "!{sys.executable} -m pip install detectron2\n",
        "\n",
        "# ================================\n",
        "# Step 3: Verify Installation\n",
        "# ================================\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "print(\"Detectron2 version:\", detectron2.__version__)\n",
        "print(\"Detectron2 installed successfully ✅\")\n"
      ],
      "metadata": {
        "id": "X5tEsR0VTVvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Annotate a dataset using any tool of your choice and convert the\n",
        "annotations to COCO format for Detectron2."
      ],
      "metadata": {
        "id": "Q4kT507qTcbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "# ================================\n",
        "# Paths (CHANGE if needed)\n",
        "# ================================\n",
        "IMAGE_DIR = \"images\"\n",
        "LABEL_DIR = \"labels\"\n",
        "OUTPUT_JSON = \"annotations_coco.json\"\n",
        "\n",
        "# ================================\n",
        "# Safety checks (ERROR FIX)\n",
        "# ================================\n",
        "if not os.path.exists(IMAGE_DIR):\n",
        "    raise FileNotFoundError(f\"Folder not found: {IMAGE_DIR}\")\n",
        "\n",
        "if not os.path.exists(LABEL_DIR):\n",
        "    raise FileNotFoundError(f\"Folder not found: {LABEL_DIR}\")\n",
        "\n",
        "# ================================\n",
        "# Class names (same order as LabelImg)\n",
        "# ================================\n",
        "CLASSES = [\"person\", \"car\", \"bike\"]\n",
        "\n",
        "coco = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "# Categories\n",
        "for i, cls in enumerate(CLASSES):\n",
        "    coco[\"categories\"].append({\n",
        "        \"id\": i + 1,\n",
        "        \"name\": cls,\n",
        "        \"supercategory\": \"object\"\n",
        "    })\n",
        "\n",
        "annotation_id = 1\n",
        "image_id = 1\n",
        "\n",
        "# ================================\n",
        "# Conversion loop\n",
        "# ================================\n",
        "for img_name in os.listdir(IMAGE_DIR):\n",
        "\n",
        "    if not img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    if img is None:\n",
        "        continue\n",
        "\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    coco[\"images\"].append({\n",
        "        \"id\": image_id,\n",
        "        \"file_name\": img_name,\n",
        "        \"height\": h,\n",
        "        \"width\": w\n",
        "    })\n",
        "\n",
        "    label_file = img_name.rsplit(\".\", 1)[0] + \".txt\"\n",
        "    label_path = os.path.join(LABEL_DIR, label_file)\n",
        "\n",
        "    if os.path.exists(label_path):\n",
        "        with open(label_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                cls_id, xc, yc, bw, bh = map(float, line.split())\n",
        "\n",
        "                x = (xc - bw / 2) * w\n",
        "                y = (yc - bh / 2) * h\n",
        "                bw *= w\n",
        "                bh *= h\n",
        "\n",
        "                coco[\"annotations\"].append({\n",
        "                    \"id\": annotation_id,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"category_id\": int(cls_id) + 1,\n",
        "                    \"bbox\": [x, y, bw, bh],\n",
        "                    \"area\": bw * bh,\n",
        "                    \"iscrowd\": 0\n",
        "                })\n",
        "\n",
        "                annotation_id += 1\n",
        "\n",
        "    image_id += 1\n",
        "\n",
        "# ================================\n",
        "# Save COCO JSON\n",
        "# ================================\n",
        "with open(OUTPUT_JSON, \"w\") as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "print(\"COCO annotations created successfully ✅\")\n",
        "print(\"Output file:\", OUTPUT_JSON)\n"
      ],
      "metadata": {
        "id": "qdgrYu7JT8dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a script to download pretrained weights and configure paths for training in Detectron2"
      ],
      "metadata": {
        "id": "_0OaaF5dT9On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 1: Import libraries\n",
        "# ================================\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# ================================\n",
        "# Step 2: Set dataset and output paths\n",
        "# ================================\n",
        "# Change these paths as per your system\n",
        "DATASET_TRAIN = \"/path/to/train/images\"\n",
        "DATASET_VAL   = \"/path/to/val/images\"\n",
        "ANNOTATIONS_TRAIN = \"/path/to/train/annotations_coco.json\"\n",
        "ANNOTATIONS_VAL   = \"/path/to/val/annotations_coco.json\"\n",
        "OUTPUT_DIR = \"./output\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# Step 3: Choose a pretrained model from Detectron2 Model Zoo\n",
        "# ================================\n",
        "# Example: Faster R-CNN with ResNet-50 backbone and FPN\n",
        "MODEL_ZOO_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "# ================================\n",
        "# Step 4: Download weights & configure training\n",
        "# ================================\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(MODEL_ZOO_CFG))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(MODEL_ZOO_CFG)  # pretrained weights\n",
        "\n",
        "# Dataset paths\n",
        "cfg.DATASETS.TRAIN = (\"my_train_dataset\",)\n",
        "cfg.DATASETS.TEST = (\"my_val_dataset\",)\n",
        "\n",
        "# Output & training settings\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust based on dataset\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Change based on number of classes\n",
        "\n",
        "# ================================\n",
        "# Step 5: Print summary\n",
        "# ================================\n",
        "print(\"Detectron2 config set up successfully ✅\")\n",
        "print(\"Pretrained weights URL:\", cfg.MODEL.WEIGHTS)\n",
        "print(\"Output directory:\", cfg.OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "8zrrYaniUOPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Show the steps and code to run inference using a trained Detectron2 model on a new image."
      ],
      "metadata": {
        "id": "4MAdrLaHUQVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 1: Import libraries\n",
        "# ================================\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# ================================\n",
        "# Step 2: Set config and load trained weights\n",
        "# ================================\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"output/config.yaml\")  # Path to trained config\n",
        "cfg.MODEL.WEIGHTS = \"output/model_final.pth\"  # Path to trained weights\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Confidence threshold\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3          # Your number of classes\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# ================================\n",
        "# Step 3: Read the image\n",
        "# ================================\n",
        "image_path = \"test_image.jpg\"  # New image path\n",
        "im = cv2.imread(image_path)\n",
        "\n",
        "# ================================\n",
        "# Step 4: Run inference\n",
        "# ================================\n",
        "outputs = predictor(im)\n",
        "\n",
        "# ================================\n",
        "# Step 5: Visualize predictions\n",
        "# ================================\n",
        "metadata = MetadataCatalog.get(\"my_dataset_val\")  # Registered dataset name\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(out.get_image())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LFZioYWzUQCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection."
      ],
      "metadata": {
        "id": "5rrGBoQQUcNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Step 1: Imports\n",
        "# ================================\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2 import model_zoo\n",
        "import json\n",
        "\n",
        "# ================================\n",
        "# Step 2: Dataset registration (COCO format)\n",
        "# ================================\n",
        "def register_coco_dataset(name, img_dir, json_file):\n",
        "    from detectron2.data.datasets import register_coco_instances\n",
        "    register_coco_instances(name, {}, json_file, img_dir)\n",
        "\n",
        "# Replace these paths with your dataset\n",
        "TRAIN_IMAGES = \"images/train\"\n",
        "VAL_IMAGES   = \"images/val\"\n",
        "TRAIN_JSON   = \"annotations/annotations_train.json\"\n",
        "VAL_JSON     = \"annotations/annotations_val.json\"\n",
        "NUM_CLASSES  = 3  # Example: deer, tiger, elephant\n",
        "\n",
        "register_coco_dataset(\"wildlife_train\", TRAIN_IMAGES, TRAIN_JSON)\n",
        "register_coco_dataset(\"wildlife_val\", VAL_IMAGES, VAL_JSON)\n",
        "\n",
        "# ================================\n",
        "# Step 3: Config and Training\n",
        "# ================================\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"wildlife_train\",)\n",
        "cfg.DATASETS.TEST  = (\"wildlife_val\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # pretrained\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000  # Adjust based on dataset\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
        "cfg.OUTPUT_DIR = \"./output_wildlife\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "# trainer.train()  # Uncomment to start training\n",
        "\n",
        "# Expected output: Logs showing loss decreasing over iterations\n",
        "# Output weights saved in ./output_wildlife/model_final.pth\n",
        "\n",
        "# ================================\n",
        "# Step 4: Inference on new image\n",
        "# ================================\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Confidence threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "test_img_path = \"images/test/test1.jpg\"\n",
        "im = cv2.imread(test_img_path)\n",
        "outputs = predictor(im)\n",
        "\n",
        "# ================================\n",
        "# Step 5: Visualization\n",
        "# ================================\n",
        "metadata = MetadataCatalog.get(\"wildlife_val\")\n",
        "v = Visualizer(im[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(out.get_image())\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Expected output: Image with bounding boxes and class labels for detected animals\n"
      ],
      "metadata": {
        "id": "Mp0izuryUie4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}